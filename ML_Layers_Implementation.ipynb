{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация основных слоев для машинного обучения\n",
    "\n",
    "В этом ноутбуке вы изучите и реализуете основные компоненты нейронных сетей:\n",
    "- Функции активации (ReLU, Sigmoid, Tanh)\n",
    "- Линейный слой (Linear)\n",
    "- Последовательный контейнер (Sequential)\n",
    "- Регуляризация (Dropout)\n",
    "- Нормализация (BatchNorm)\n",
    "\n",
    "Каждый блок содержит теоретическое объяснение, шаблон для реализации и тесты для проверки.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:31.991235Z",
     "start_time": "2025-08-12T18:07:31.988236Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, List\n",
    "\n",
    "# Установим seed для воспроизводимости\n",
    "np.random.seed(42)\n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Базовый класс для всех слоев\n",
    "\n",
    "Сначала определим базовый класс, от которого будут наследоваться все наши слои:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:33.453368Z",
     "start_time": "2025-08-12T18:07:33.449342Z"
    }
   },
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Базовый класс для всех слоев нейронной сети\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямое распространение\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Обратное распространение\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Переключение в режим обучения\n",
    "        \"\"\"\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Переключение в режим инференса\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Функция активации ReLU\n",
    "\n",
    "### Теория\n",
    "\n",
    "**ReLU (Rectified Linear Unit)** - одна из самых популярных функций активации в современных нейронных сетях.\n",
    "\n",
    "**Формула:** \n",
    "- Forward: `f(x) = max(0, x)`\n",
    "- Backward: `df/dx = 1 если x > 0, иначе 0`\n",
    "\n",
    "**Преимущества:**\n",
    "- Простота вычислений\n",
    "- Решает проблему затухающих градиентов\n",
    "- Разреженность активаций\n",
    "\n",
    "**Недостатки:**\n",
    "- \"Мертвые нейроны\" (dying ReLU problem)\n",
    "\n",
    "### Реализация\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:08:44.660931Z",
     "start_time": "2025-08-12T18:08:44.656907Z"
    }
   },
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямое распространение для ReLU\n",
    "        \n",
    "        Args:\n",
    "            x: входной тензор формы (batch_size, ...)\n",
    "        \n",
    "        Returns:\n",
    "            выходной тензор той же формы\n",
    "        \"\"\"\n",
    "        self.input = x\n",
    "        \n",
    "        output = np.maximum(0, x)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Обратное распространение для ReLU\n",
    "        \n",
    "        Args:\n",
    "            grad_output: градиент от следующего слоя\n",
    "        \n",
    "        Returns:\n",
    "            градиент для предыдущего слоя\n",
    "        \"\"\"\n",
    "        grad_input = grad_output * (self.input > 0).astype(grad_output.dtype)\n",
    "        \n",
    "        return grad_input\n"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:08:47.019787Z",
     "start_time": "2025-08-12T18:08:47.012972Z"
    }
   },
   "source": [
    "# Тест ReLU (запустите этот код после реализации ReLU)\n",
    "relu = ReLU()\n",
    "\n",
    "# Тестовые данные\n",
    "x_test = np.array([[-2, -1, 0, 1, 2]], dtype=np.float32)\n",
    "expected_forward = np.array([[0, 0, 0, 1, 2]], dtype=np.float32)\n",
    "\n",
    "# Forward pass\n",
    "output = relu.forward(x_test)\n",
    "print(f\"Input: {x_test}\")\n",
    "print(f\"Output: {output}\")\n",
    "print(f\"Expected: {expected_forward}\")\n",
    "\n",
    "# Проверка forward pass\n",
    "assert np.allclose(output, expected_forward), \"ReLU forward pass не работает корректно!\"\n",
    "\n",
    "# Backward pass\n",
    "grad_output = np.ones_like(output)\n",
    "grad_input = relu.backward(grad_output)\n",
    "expected_backward = np.array([[0, 0, 0, 1, 1]], dtype=np.float32)\n",
    "\n",
    "print(f\"Gradient output: {grad_output}\")\n",
    "print(f\"Gradient input: {grad_input}\")\n",
    "print(f\"Expected gradient: {expected_backward}\")\n",
    "\n",
    "# Проверка backward pass\n",
    "assert np.allclose(grad_input, expected_backward), \"ReLU backward pass не работает корректно!\"\n",
    "\n",
    "print(\"✅ ReLU тест пройден успешно!\")\n",
    "\n",
    "print(\"⚠️ Реализуйте ReLU класс выше, затем раскомментируйте этот код для тестирования\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[-2. -1.  0.  1.  2.]]\n",
      "Output: [[0. 0. 0. 1. 2.]]\n",
      "Expected: [[0. 0. 0. 1. 2.]]\n",
      "Gradient output: [[1. 1. 1. 1. 1.]]\n",
      "Gradient input: [[0. 0. 0. 1. 1.]]\n",
      "Expected gradient: [[0. 0. 0. 1. 1.]]\n",
      "✅ ReLU тест пройден успешно!\n",
      "⚠️ Реализуйте ReLU класс выше, затем раскомментируйте этот код для тестирования\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Функция активации Sigmoid\n",
    "\n",
    "### Теория\n",
    "\n",
    "**Sigmoid** - классическая функция активации, которая \"сжимает\" входные значения в диапазон (0, 1).\n",
    "\n",
    "**Формула:**\n",
    "- Forward: `f(x) = 1 / (1 + exp(-x))`\n",
    "- Backward: `df/dx = f(x) * (1 - f(x))`\n",
    "\n",
    "**Применение:**\n",
    "- Бинарная классификация (выходной слой)\n",
    "- Gating механизмы (LSTM, GRU)\n",
    "\n",
    "**Проблемы:**\n",
    "- Затухающие градиенты при глубоких сетях\n",
    "- Насыщение при больших значениях\n",
    "\n",
    "### Реализация\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:08:51.787668Z",
     "start_time": "2025-08-12T18:08:51.782643Z"
    }
   },
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямое распространение для Sigmoid\n",
    "        \n",
    "        Args:\n",
    "            x: входной тензор\n",
    "        \n",
    "        Returns:\n",
    "            выходной тензор той же формы, значения в диапазоне (0, 1)\n",
    "        \"\"\"\n",
    "        self.output = 1 / (1  + np.exp(-x))\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Обратное распространение для Sigmoid\n",
    "        \n",
    "        Args:\n",
    "            grad_output: градиент от следующего слоя\n",
    "        \n",
    "        Returns:\n",
    "            градиент для предыдущего слоя\n",
    "        \"\"\"\n",
    "        grad_input = grad_output * (1 - self.output) * self.output\n",
    "        \n",
    "        return grad_input\n"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование Sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T13:26:18.997078Z",
     "start_time": "2025-08-11T13:26:18.991343Z"
    }
   },
   "source": [
    "# Тест Sigmoid (запустите этот код после реализации Sigmoid)\n",
    "print(\"⚠️ Реализуйте Sigmoid класс выше, затем раскомментируйте этот код для тестирования\")\n",
    "\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "# Тестовые данные\n",
    "x_test = np.array([[-10, -1, 0, 1, 10]], dtype=np.float32)\n",
    "\n",
    "# Forward pass\n",
    "output = sigmoid.forward(x_test)\n",
    "print(f\"Input: {x_test}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "# Проверим, что выходные значения в диапазоне (0, 1)\n",
    "assert np.all(output > 0) and np.all(output < 1), \"Sigmoid должен возвращать значения в диапазоне (0, 1)\"\n",
    "\n",
    "# Проверим симметричность: sigmoid(-x) = 1 - sigmoid(x)\n",
    "x_sym = np.array([[1]], dtype=np.float32)\n",
    "out_pos = sigmoid.forward(x_sym)\n",
    "out_neg = sigmoid.forward(-x_sym)\n",
    "assert np.allclose(out_neg, 1 - out_pos, atol=1e-6), \"Sigmoid должен быть симметричным\"\n",
    "\n",
    "# Backward pass\n",
    "grad_output = np.ones_like(output)\n",
    "grad_input = sigmoid.backward(grad_output)\n",
    "print(f\"Gradient input: {grad_input}\")\n",
    "\n",
    "# Проверим, что градиент положительный (sigmoid монотонно возрастает)\n",
    "assert np.all(grad_input >= 0), \"Градиент Sigmoid должен быть неотрицательным\"\n",
    "\n",
    "print(\"✅ Sigmoid тест пройден успешно!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Реализуйте Sigmoid класс выше, затем раскомментируйте этот код для тестирования\n",
      "Input: [[-10.  -1.   0.   1.  10.]]\n",
      "Output: [[4.539787e-05 2.689414e-01 5.000000e-01 7.310586e-01 9.999546e-01]]\n",
      "Gradient input: [[0.19661193 0.19661193 0.19661193 0.19661193 0.19661193]]\n",
      "✅ Sigmoid тест пройден успешно!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Функция активации Tanh\n",
    "\n",
    "### Теория\n",
    "\n",
    "**Tanh (гиперболический тангенс)** - функция активации, которая \"сжимает\" входные значения в диапазон (-1, 1).\n",
    "\n",
    "**Формула:**\n",
    "- Forward: `f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`\n",
    "- Backward: `df/dx = 1 - f(x)²`\n",
    "\n",
    "**Преимущества над Sigmoid:**\n",
    "- Выход центрирован вокруг нуля\n",
    "- Больший диапазон градиентов\n",
    "- Связь с sigmoid: `tanh(x) = 2*sigmoid(2x) - 1`\n",
    "\n",
    "### Реализация\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:08:54.412389Z",
     "start_time": "2025-08-12T18:08:54.408837Z"
    }
   },
   "source": [
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямое распространение для Tanh\n",
    "        \n",
    "        Args:\n",
    "            x: входной тензор\n",
    "        \n",
    "        Returns:\n",
    "            выходной тензор той же формы, значения в диапазоне (-1, 1)\n",
    "        \"\"\"\n",
    "        self.output = np.tanh(x)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Обратное распространение для Tanh\n",
    "        \n",
    "        Args:\n",
    "            grad_output: градиент от следующего слоя\n",
    "        \n",
    "        Returns:\n",
    "            градиент для предыдущего слоя\n",
    "        \"\"\"\n",
    "        grad_input = (1 - self.output ** 2) * grad_output\n",
    "        \n",
    "        return grad_input\n"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование Tanh\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:34.670084Z",
     "start_time": "2025-08-12T18:07:34.666835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tanh = Tanh()\n",
    "x_test = np.array([[-10, -1, 0, 1, 10]], dtype=np.float32)\n",
    "print(x_test[0][0])\n",
    "print(tanh.forward(x_test[0][0]))\n",
    "print(tanh.forward(-10.0))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.0\n",
      "-1.0\n",
      "-0.9999999958776927\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:34.814576Z",
     "start_time": "2025-08-12T18:07:34.807451Z"
    }
   },
   "source": [
    "# Тест Tanh (запустите этот код после реализации Tanh)\n",
    "print(\"⚠️ Реализуйте Tanh класс выше, затем раскомментируйте этот код для тестирования\")\n",
    "\n",
    "tanh = Tanh()\n",
    "\n",
    "# Тестовые данные\n",
    "x_test = np.array([[-10, -1, 0, 1, 10]], dtype=np.int32)\n",
    "\n",
    "# Forward pass\n",
    "output = tanh.forward(x_test)\n",
    "print(f\"Input: {x_test}\")\n",
    "print(f\"Output: {output}\")\n",
    "\n",
    "\n",
    "# Проверим, что выходные значения в диапазоне (-1, 1)\n",
    "assert np.all(output > -1) and np.all(output < 1), \"Tanh должен возвращать значения в диапазоне (-1, 1)\"\n",
    "\n",
    "\n",
    "# Проверим антисимметричность: tanh(-x) = -tanh(x)\n",
    "x_antisym = np.array([[2]], dtype=np.float32)\n",
    "out_pos = tanh.forward(x_antisym)\n",
    "out_neg = tanh.forward(-x_antisym)\n",
    "assert np.allclose(out_neg, -out_pos, atol=1e-6), \"Tanh должен быть антисимметричным\"\n",
    "\n",
    "# Проверим, что tanh(0) = 0\n",
    "zero_out = tanh.forward(np.array([[0]], dtype=np.float32))\n",
    "assert np.allclose(zero_out, 0, atol=1e-6), \"tanh(0) должен быть равен 0\"\n",
    "\n",
    "# Backward pass\n",
    "grad_output = np.ones_like(output)\n",
    "grad_input = tanh.backward(grad_output)\n",
    "print(f\"Gradient input: {grad_input}\")\n",
    "\n",
    "# Проверим, что градиент положительный (tanh монотонно возрастает)\n",
    "assert np.all(grad_input >= 0), \"Градиент Tanh должен быть неотрицательным\"\n",
    "\n",
    "print(\"✅ Tanh тест пройден успешно!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Реализуйте Tanh класс выше, затем раскомментируйте этот код для тестирования\n",
      "Input: [[-10  -1   0   1  10]]\n",
      "Output: [[-1.         -0.76159416  0.          0.76159416  1.        ]]\n",
      "Gradient input: [[1. 1. 1. 1. 1.]]\n",
      "✅ Tanh тест пройден успешно!\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Линейный слой (Linear/Dense)\n",
    "\n",
    "### Теория\n",
    "\n",
    "**Линейный слой** - основной строительный блок нейронных сетей, выполняющий аффинное преобразование.\n",
    "\n",
    "**Формула:**\n",
    "- Forward: `y = x @ W + b`\n",
    "- где W - матрица весов размера (input_size, output_size)\n",
    "- b - вектор смещений размера (output_size,)\n",
    "\n",
    "**Градиенты:**\n",
    "- `∂L/∂x = grad_output @ W.T`\n",
    "- `∂L/∂W = x.T @ grad_output`\n",
    "- `∂L/∂b = sum(grad_output, axis=0)`\n",
    "\n",
    "**Инициализация весов:**\n",
    "- Xavier/Glorot: помогает поддерживать дисперсию активаций\n",
    "- He: оптимизирована для ReLU активаций\n",
    "\n",
    "### Реализация\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:35.112178Z",
     "start_time": "2025-08-12T18:07:35.106334Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_size, output_size, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.use_bias = bias\n",
    "\n",
    "        limit = np.sqrt(6 / (input_size + output_size))\n",
    "        self.weight = np.random.uniform(-limit, limit, (input_size, output_size))\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.bias = np.zeros(output_size)\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        self.input = None\n",
    "        self.grad_weight = None\n",
    "        self.grad_bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямое распространение для линейного слоя\n",
    "        \"\"\"\n",
    "        self.input = x\n",
    "        output = x @ self.weight\n",
    "\n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Обратное распространение для линейного слоя\n",
    "        \"\"\"\n",
    "        grad_input = grad_output @ self.weight.T\n",
    "\n",
    "        self.grad_weight = self.input.T @ grad_output  # (input_size, output_size)\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.grad_bias = grad_output.sum(axis=0)  # (output_size,)\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "    def update_weights(self, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Обновление весов с помощью градиентного спуска\n",
    "        \"\"\"\n",
    "        if self.grad_weight is not None:\n",
    "            self.weight -= learning_rate * self.grad_weight\n",
    "        \n",
    "        if self.use_bias and self.grad_bias is not None:\n",
    "            self.bias -= learning_rate * self.grad_bias\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование Linear\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:35.421494Z",
     "start_time": "2025-08-12T18:07:35.413722Z"
    }
   },
   "source": [
    "# Тест Linear (запустите этот код после реализации Linear)\n",
    "print(\"⚠️ Реализуйте Linear класс выше, затем раскомментируйте этот код для тестирования\")\n",
    "\n",
    "linear = Linear(input_size=3, output_size=2, bias=True)\n",
    "\n",
    "# Проверим форму весов\n",
    "assert linear.weight.shape == (3, 2), f\"Неправильная форма весов: {linear.weight.shape}\"\n",
    "assert linear.bias.shape == (2,), f\"Неправильная форма bias: {linear.bias.shape}\"\n",
    "\n",
    "print(f\"Веса: \\n{linear.weight}\")\n",
    "print(f\"Bias: {linear.bias}\")\n",
    "\n",
    "# Тестовые данные\n",
    "batch_size = 4\n",
    "x_test = np.random.randn(batch_size, 3).astype(np.float32)\n",
    "\n",
    "# Forward pass\n",
    "output = linear.forward(x_test)\n",
    "expected_shape = (batch_size, 2)\n",
    "\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected shape: {expected_shape}\")\n",
    "\n",
    "assert output.shape == expected_shape, f\"Неправильная форма выхода: {output.shape}\"\n",
    "\n",
    "# Backward pass\n",
    "grad_output = np.random.randn(*output.shape).astype(np.float32)\n",
    "grad_input = linear.backward(grad_output)\n",
    "\n",
    "print(f\"Gradient input shape: {grad_input.shape}\")\n",
    "print(f\"Gradient weight shape: {linear.grad_weight.shape}\")\n",
    "print(f\"Gradient bias shape: {linear.grad_bias.shape}\")\n",
    "\n",
    "# Проверим формы градиентов\n",
    "assert grad_input.shape == x_test.shape, \"Неправильная форма градиента по входу\"\n",
    "assert linear.grad_weight.shape == linear.weight.shape, \"Неправильная форма градиента по весам\"\n",
    "assert linear.grad_bias.shape == linear.bias.shape, \"Неправильная форма градиента по bias\"\n",
    "\n",
    "print(\"✅ Linear тест пройден успешно!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Реализуйте Linear класс выше, затем раскомментируйте этот код для тестирования\n",
      "Веса: \n",
      "[[-0.27486883  0.98746557]\n",
      " [ 0.50827326  0.21614991]\n",
      " [-0.7536254  -0.75367824]]\n",
      "Bias: [0. 0.]\n",
      "Input shape: (4, 3)\n",
      "Output shape: (4, 2)\n",
      "Expected shape: (4, 2)\n",
      "Gradient input shape: (4, 3)\n",
      "Gradient weight shape: (3, 2)\n",
      "Gradient bias shape: (2,)\n",
      "✅ Linear тест пройден успешно!\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Последовательный контейнер (Sequential)\n",
    "\n",
    "### Теория\n",
    "\n",
    "**Sequential** - контейнер, который позволяет последовательно применять несколько слоев.\n",
    "\n",
    "**Принцип работы:**\n",
    "- Forward: применяет слои по порядку: `output = layer_n(...layer_2(layer_1(input))...)`\n",
    "- Backward: применяет градиенты в обратном порядке\n",
    "\n",
    "**Применение:**\n",
    "- Создание простых feed-forward сетей\n",
    "- Группировка слоев в блоки\n",
    "- Упрощение архитектуры кода\n",
    "\n",
    "### Реализация\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:35.693988Z",
     "start_time": "2025-08-12T18:07:35.687457Z"
    }
   },
   "source": [
    "class Sequential(Layer):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.layers = list(layers)\n",
    "        self.layer_outputs = []\n",
    "    \n",
    "    def add(self, layer):\n",
    "        \"\"\"\n",
    "        Добавление слоя в последовательность\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямое распространение через все слои\n",
    "\n",
    "        Args:\n",
    "            x: входной тензор\n",
    "\n",
    "        Returns:\n",
    "            выходной тензор после прохождения всех слоев\n",
    "        \"\"\"\n",
    "        self.layer_outputs = []\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "            self.layer_outputs.append(output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Обратное распространение через все слои в обратном порядке\n",
    "\n",
    "        Args:\n",
    "            grad_output: градиент от следующего слоя\n",
    "\n",
    "        Returns:\n",
    "            градиент для предыдущего слоя\n",
    "        \"\"\"\n",
    "        grad = grad_output\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Переключение всех слоев в режим обучения\n",
    "        \"\"\"\n",
    "        super().train()\n",
    "        for layer in self.layers:\n",
    "            layer.train()\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Переключение всех слоев в режим инференса\n",
    "        \"\"\"\n",
    "        super().eval()\n",
    "        for layer in self.layers:\n",
    "            layer.eval()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.layers)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.layers[idx]\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dropout\n",
    "\n",
    "### Теория\n",
    "\n",
    "**Dropout** - техника регуляризации, которая случайно \"выключает\" некоторые нейроны во время обучения.\n",
    "\n",
    "**Принцип работы:**\n",
    "- **Обучение**: каждый нейрон сохраняется с вероятностью `(1 - dropout_rate)`\n",
    "- **Инференс**: все нейроны активны, но выходы масштабируются\n",
    "\n",
    "**Преимущества:**\n",
    "- Предотвращает переобучение\n",
    "- Улучшает обобщающую способность\n",
    "- Эффект ансамбля моделей\n",
    "\n",
    "### Реализация\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:35.985789Z",
     "start_time": "2025-08-12T18:07:35.980976Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямое распространение для Dropout\n",
    "\n",
    "        Args:\n",
    "            x: входной тензор\n",
    "\n",
    "        Returns:\n",
    "            выходной тензор с примененным dropout (в режиме обучения)\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self.mask = (np.random.rand(*x.shape) >= self.dropout_rate).astype(x.dtype)\n",
    "            output = (x * self.mask) / (1.0 - self.dropout_rate)\n",
    "        else:\n",
    "            output = x\n",
    "            self.mask = None\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Обратное распространение для Dropout\n",
    "\n",
    "        Args:\n",
    "            grad_output: градиент от следующего слоя\n",
    "\n",
    "        Returns:\n",
    "            градиент для предыдущего слоя\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            grad_input = (grad_output * self.mask) / (1.0 - self.dropout_rate)\n",
    "        else:\n",
    "            grad_input = grad_output\n",
    "        return grad_input\n"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:36.277303Z",
     "start_time": "2025-08-12T18:07:36.269764Z"
    }
   },
   "source": [
    "# Тест Dropout (запустите после реализации Dropout)\n",
    "print(\"⚠️ Реализуйте Dropout класс выше, затем раскомментируйте этот код для тестирования\")\n",
    "\n",
    "dropout = Dropout(dropout_rate=0.5)\n",
    "\n",
    "# Тестовые данные\n",
    "x_test = np.ones((100, 10), dtype=np.float32)\n",
    "\n",
    "# Тест в режиме обучения\n",
    "dropout.train()\n",
    "output_train = dropout.forward(x_test)\n",
    "\n",
    "print(f\"Режим обучения:\")\n",
    "print(f\"Input mean: {x_test.mean():.3f}\")\n",
    "print(f\"Output mean: {output_train.mean():.3f}\")\n",
    "print(f\"Proportion of zeros: {(output_train == 0).mean():.3f}\")\n",
    "\n",
    "# Проверим, что часть нейронов \"выключена\"\n",
    "zeros_ratio = (output_train == 0).mean()\n",
    "expected_zeros = 0.5  # dropout_rate\n",
    "assert abs(zeros_ratio - expected_zeros) < 0.1, f\"Неправильная доля нулевых значений: {zeros_ratio}\"\n",
    "\n",
    "# Проверим масштабирование\n",
    "assert abs(output_train.mean() - x_test.mean()) < 0.1, \"Неправильное масштабирование в режиме обучения\"\n",
    "\n",
    "# Тест в режиме инференса\n",
    "dropout.eval()\n",
    "output_eval = dropout.forward(x_test)\n",
    "\n",
    "print(f\"\\nРежим инференса:\")\n",
    "print(f\"Output mean: {output_eval.mean():.3f}\")\n",
    "print(f\"Proportion of zeros: {(output_eval == 0).mean():.3f}\")\n",
    "\n",
    "# В режиме инференса все значения должны остаться\n",
    "assert np.allclose(output_eval, x_test), \"В режиме инференса выход должен совпадать с входом\"\n",
    "\n",
    "# Тест backward pass\n",
    "dropout.train()\n",
    "output_train = dropout.forward(x_test)\n",
    "grad_output = np.ones_like(output_train)\n",
    "grad_input = dropout.backward(grad_output)\n",
    "\n",
    "print(f\"\\nGradient test:\")\n",
    "print(f\"Grad input shape: {grad_input.shape}\")\n",
    "print(f\"Grad input mean: {grad_input.mean():.3f}\")\n",
    "\n",
    "assert grad_input.shape == x_test.shape, \"Неправильная форма градиента\"\n",
    "\n",
    "print(\"✅ Dropout тест пройден успешно!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Реализуйте Dropout класс выше, затем раскомментируйте этот код для тестирования\n",
      "Режим обучения:\n",
      "Input mean: 1.000\n",
      "Output mean: 1.008\n",
      "Proportion of zeros: 0.496\n",
      "\n",
      "Режим инференса:\n",
      "Output mean: 1.000\n",
      "Proportion of zeros: 0.000\n",
      "\n",
      "Gradient test:\n",
      "Grad input shape: (100, 10)\n",
      "Grad input mean: 1.018\n",
      "✅ Dropout тест пройден успешно!\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Normalization\n",
    "\n",
    "### Теория\n",
    "\n",
    "**Batch Normalization** - техника нормализации, которая стабилизирует обучение глубоких сетей.\n",
    "\n",
    "**Принцип работы:**\n",
    "1. Нормализация: `(x - mean) / sqrt(var + eps)`\n",
    "2. Масштабирование и сдвиг: `gamma * normalized + beta`\n",
    "\n",
    "**Преимущества:**\n",
    "- Ускоряет обучение\n",
    "- Позволяет использовать большие learning rate\n",
    "- Уменьшает зависимость от инициализации\n",
    "- Эффект регуляризации\n",
    "\n",
    "**Различия между режимами:**\n",
    "- **Обучение**: используется статистика текущего batch\n",
    "- **Инференс**: используется накопленная статистика\n",
    "\n",
    "### Реализация\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:36.610267Z",
     "start_time": "2025-08-12T18:07:36.603243Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchNorm(Layer):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        self.gamma = np.ones(num_features)\n",
    "        self.beta = np.zeros(num_features)\n",
    "\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "\n",
    "        self.batch_mean = None\n",
    "        self.batch_var = None\n",
    "        self.normalized = None\n",
    "        self.input = None\n",
    "        self.grad_gamma = None\n",
    "        self.grad_beta = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Прямое распространение для Batch Normalization\n",
    "\n",
    "        Args:\n",
    "            x: входной тензор формы (batch_size, num_features)\n",
    "\n",
    "        Returns:\n",
    "            нормализованный выходной тензор той же формы\n",
    "        \"\"\"\n",
    "        self.input = x\n",
    "\n",
    "        if self.training:\n",
    "            self.batch_mean = np.mean(x, axis=0)\n",
    "            self.batch_var = np.var(x, axis=0)\n",
    "\n",
    "            self.running_mean = self.momentum * self.batch_mean + (1 - self.momentum) * self.running_mean\n",
    "            self.running_var = self.momentum * self.batch_var + (1 - self.momentum) * self.running_var\n",
    "\n",
    "            mean = self.batch_mean\n",
    "            var = self.batch_var\n",
    "        else:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "\n",
    "        self.normalized = (x - mean) / np.sqrt(var + self.eps)\n",
    "        output = self.gamma * self.normalized + self.beta\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Обратное распространение для Batch Normalization\n",
    "        \"\"\"\n",
    "        batch_size = self.input.shape[0]\n",
    "\n",
    "        self.grad_gamma = np.sum(grad_output * self.normalized, axis=0)\n",
    "        self.grad_beta = np.sum(grad_output, axis=0)\n",
    "\n",
    "        grad_norm = grad_output * self.gamma\n",
    "        var_inv = 1.0 / np.sqrt(self.batch_var + self.eps)\n",
    "\n",
    "        grad_input = (1. / batch_size) * var_inv * (\n",
    "            batch_size * grad_norm\n",
    "            - np.sum(grad_norm, axis=0)\n",
    "            - self.normalized * np.sum(grad_norm * self.normalized, axis=0)\n",
    "        )\n",
    "        \n",
    "        return grad_input\n",
    "    \n",
    "    def update_weights(self, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Обновление параметров\n",
    "        \"\"\"\n",
    "        if self.grad_gamma is not None:\n",
    "            self.gamma -= learning_rate * self.grad_gamma\n",
    "        \n",
    "        if self.grad_beta is not None:\n",
    "            self.beta -= learning_rate * self.grad_beta\n"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:36.755425Z",
     "start_time": "2025-08-12T18:07:36.746069Z"
    }
   },
   "source": [
    "# Тест BatchNorm (запустите после реализации BatchNorm)\n",
    "print(\"⚠️ Реализуйте BatchNorm класс выше, затем раскомментируйте этот код для тестирования\")\n",
    "\n",
    "batch_norm = BatchNorm(num_features=4)\n",
    "\n",
    "# Проверим инициализацию\n",
    "assert np.allclose(batch_norm.gamma, 1.0), \"Gamma должно инициализироваться единицами\"\n",
    "assert np.allclose(batch_norm.beta, 0.0), \"Beta должно инициализироваться нулями\"\n",
    "assert np.allclose(batch_norm.running_mean, 0.0), \"Running mean должно инициализироваться нулями\"\n",
    "assert np.allclose(batch_norm.running_var, 1.0), \"Running var должно инициализироваться единицами\"\n",
    "\n",
    "print(f\"Gamma: {batch_norm.gamma}\")\n",
    "print(f\"Beta: {batch_norm.beta}\")\n",
    "\n",
    "# Тестовые данные с известной статистикой\n",
    "x_test = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 3, 4, 5],\n",
    "    [3, 4, 5, 6]\n",
    "], dtype=np.float32)\n",
    "\n",
    "print(f\"Input: \\n{x_test}\")\n",
    "print(f\"Input mean per feature: {x_test.mean(axis=0)}\")\n",
    "print(f\"Input std per feature: {x_test.std(axis=0)}\")\n",
    "\n",
    "# Forward pass в режиме обучения\n",
    "batch_norm.train()\n",
    "output = batch_norm.forward(x_test)\n",
    "\n",
    "print(f\"\\nOutput: \\n{output}\")\n",
    "print(f\"Output mean per feature: {output.mean(axis=0)}\")\n",
    "print(f\"Output std per feature: {output.std(axis=0)}\")\n",
    "\n",
    "# Проверим, что выход нормализован (среднее ≈ 0, std ≈ 1)\n",
    "assert np.allclose(output.mean(axis=0), 0, atol=1e-6), \"Среднее должно быть близко к 0\"\n",
    "assert np.allclose(output.std(axis=0), 1, atol=1e-6), \"Стандартное отклонение должно быть близко к 1\"\n",
    "\n",
    "# Проверим обновление running статистики\n",
    "print(f\"\\nRunning mean: {batch_norm.running_mean}\")\n",
    "print(f\"Running var: {batch_norm.running_var}\")\n",
    "\n",
    "# Backward pass\n",
    "grad_output = np.ones_like(output)\n",
    "grad_input = batch_norm.backward(grad_output)\n",
    "\n",
    "print(f\"\\nGradient input shape: {grad_input.shape}\")\n",
    "print(f\"Gradient gamma shape: {batch_norm.grad_gamma.shape}\")\n",
    "print(f\"Gradient beta shape: {batch_norm.grad_beta.shape}\")\n",
    "\n",
    "assert grad_input.shape == x_test.shape, \"Неправильная форма градиента по входу\"\n",
    "assert batch_norm.grad_gamma.shape == batch_norm.gamma.shape, \"Неправильная форма градиента gamma\"\n",
    "assert batch_norm.grad_beta.shape == batch_norm.beta.shape, \"Неправильная форма градиента beta\"\n",
    "\n",
    "# Тест режима инференса\n",
    "batch_norm.eval()\n",
    "output_eval = batch_norm.forward(x_test)\n",
    "print(f\"\\nInference mode output mean: {output_eval.mean(axis=0)}\")\n",
    "\n",
    "print(\"✅ BatchNorm тест пройден успешно!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Реализуйте BatchNorm класс выше, затем раскомментируйте этот код для тестирования\n",
      "Gamma: [1. 1. 1. 1.]\n",
      "Beta: [0. 0. 0. 0.]\n",
      "Input: \n",
      "[[1. 2. 3. 4.]\n",
      " [2. 3. 4. 5.]\n",
      " [3. 4. 5. 6.]]\n",
      "Input mean per feature: [2. 3. 4. 5.]\n",
      "Input std per feature: [0.8164966 0.8164966 0.8164966 0.8164966]\n",
      "\n",
      "Output: \n",
      "[[-1.22473562 -1.22473562 -1.22473562 -1.22473562]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 1.22473562  1.22473562  1.22473562  1.22473562]]\n",
      "Output mean per feature: [0. 0. 0. 0.]\n",
      "Output std per feature: [0.99999244 0.99999244 0.99999244 0.99999244]\n",
      "\n",
      "Running mean: [0.2        0.30000001 0.40000001 0.5       ]\n",
      "Running var: [0.96666667 0.96666667 0.96666667 0.96666667]\n",
      "\n",
      "Gradient input shape: (3, 4)\n",
      "Gradient gamma shape: (4,)\n",
      "Gradient beta shape: (4,)\n",
      "\n",
      "Inference mode output mean: [1.83076198 2.74614297 3.66152397 4.57690497]\n",
      "✅ BatchNorm тест пройден успешно!\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Adam Оптимизатор\n",
    "\n",
    "### Теория\n",
    "\n",
    "**Adam (Adaptive Moment Estimation)** - современный оптимизатор, сочетающий преимущества RMSprop и Momentum.\n",
    "\n",
    "**Принцип работы:**\n",
    "1. Вычисление экспоненциального скользящего среднего градиентов (momentum)\n",
    "2. Вычисление экспоненциального скользящего среднего квадратов градиентов (RMSprop)\n",
    "3. Коррекция смещения (bias correction)\n",
    "4. Обновление параметров\n",
    "\n",
    "**Преимущества:**\n",
    "- Адаптивные learning rate для каждого параметра\n",
    "- Хорошо работает на практике\n",
    "- Требует мало настройки гиперпараметров\n",
    "\n",
    "### Реализация\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:37.037634Z",
     "start_time": "2025-08-12T18:07:37.030579Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, layer, layer_id):\n",
    "        \"\"\"\n",
    "        Обновление параметров слоя с помощью Adam\n",
    "\n",
    "        Args:\n",
    "            layer: слой с градиентами\n",
    "            layer_id: уникальный идентификатор слоя\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "\n",
    "        if hasattr(layer, 'grad_weight') and layer.grad_weight is not None:\n",
    "            if f\"{layer_id}_weight\" not in self.m:\n",
    "                self.m[f\"{layer_id}_weight\"] = np.zeros_like(layer.grad_weight)\n",
    "                self.v[f\"{layer_id}_weight\"] = np.zeros_like(layer.grad_weight)\n",
    "\n",
    "            self.m[f\"{layer_id}_weight\"] = self.beta1 * self.m[f\"{layer_id}_weight\"] + (1 - self.beta1) * layer.grad_weight\n",
    "            self.v[f\"{layer_id}_weight\"] = self.beta2 * self.v[f\"{layer_id}_weight\"] + (1 - self.beta2) * (layer.grad_weight ** 2)\n",
    "\n",
    "            m_corrected = self.m[f\"{layer_id}_weight\"] / (1 - self.beta1 ** self.t)\n",
    "            v_corrected = self.v[f\"{layer_id}_weight\"] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            layer.weight -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.eps)\n",
    "\n",
    "        if hasattr(layer, 'grad_bias') and layer.grad_bias is not None:\n",
    "            if f\"{layer_id}_bias\" not in self.m:\n",
    "                self.m[f\"{layer_id}_bias\"] = np.zeros_like(layer.grad_bias)\n",
    "                self.v[f\"{layer_id}_bias\"] = np.zeros_like(layer.grad_bias)\n",
    "\n",
    "            self.m[f\"{layer_id}_bias\"] = self.beta1 * self.m[f\"{layer_id}_bias\"] + (1 - self.beta1) * layer.grad_bias\n",
    "            self.v[f\"{layer_id}_bias\"] = self.beta2 * self.v[f\"{layer_id}_bias\"] + (1 - self.beta2) * (layer.grad_bias ** 2)\n",
    "\n",
    "            m_corrected = self.m[f\"{layer_id}_bias\"] / (1 - self.beta1 ** self.t)\n",
    "            v_corrected = self.v[f\"{layer_id}_bias\"] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            layer.bias -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.eps)\n",
    "    \n",
    "    def zero_grad(self, layers):\n",
    "        \"\"\"\n",
    "        Обнуление градиентов\n",
    "        \"\"\"\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'grad_weight'):\n",
    "                layer.grad_weight = None\n",
    "            if hasattr(layer, 'grad_bias'):\n",
    "                layer.grad_bias = None\n",
    "            if hasattr(layer, 'grad_gamma'):\n",
    "                layer.grad_gamma = None\n",
    "            if hasattr(layer, 'grad_beta'):\n",
    "                layer.grad_beta = None\n"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:37.186980Z",
     "start_time": "2025-08-12T18:07:37.177831Z"
    }
   },
   "source": [
    "# Тест Adam (запустите после реализации Adam)\n",
    "print(\"⚠️ Реализуйте Adam класс выше, затем раскомментируйте этот код для тестирования\")\n",
    "\n",
    "# Создание тестового слоя\n",
    "layer = Linear(3, 2)\n",
    "adam = Adam(learning_rate=0.01)\n",
    "\n",
    "# Создание фиктивных градиентов\n",
    "layer.grad_weight = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], dtype=np.float32)\n",
    "layer.grad_bias = np.array([0.1, 0.2], dtype=np.float32)\n",
    "\n",
    "# Сохранение начальных весов\n",
    "initial_weight = layer.weight.copy()\n",
    "initial_bias = layer.bias.copy()\n",
    "\n",
    "print(f\"Initial weight: \\n{initial_weight}\")\n",
    "print(f\"Initial bias: {initial_bias}\")\n",
    "print(f\"Weight gradient: \\n{layer.grad_weight}\")\n",
    "print(f\"Bias gradient: {layer.grad_bias}\")\n",
    "\n",
    "# Проверим инициализацию Adam\n",
    "assert len(adam.m) == 0, \"Моменты должны быть пустыми при инициализации\"\n",
    "assert len(adam.v) == 0, \"Моменты должны быть пустыми при инициализации\"\n",
    "assert adam.t == 0, \"Time step должен быть равен 0\"\n",
    "\n",
    "# Выполним один шаг оптимизации\n",
    "adam.update(layer, \"test_layer\")\n",
    "\n",
    "print(f\"\\nAfter 1 step:\")\n",
    "print(f\"Updated weight: \\n{layer.weight}\")\n",
    "print(f\"Updated bias: {layer.bias}\")\n",
    "print(f\"Time step: {adam.t}\")\n",
    "\n",
    "# Проверим, что веса изменились\n",
    "assert not np.allclose(layer.weight, initial_weight), \"Веса должны измениться после обновления\"\n",
    "assert not np.allclose(layer.bias, initial_bias), \"Bias должен измениться после обновления\"\n",
    "\n",
    "# Проверим, что моменты инициализированы\n",
    "assert \"test_layer_weight\" in adam.m, \"Момент для весов должен быть создан\"\n",
    "assert \"test_layer_bias\" in adam.m, \"Момент для bias должен быть создан\"\n",
    "assert \"test_layer_weight\" in adam.v, \"Момент для весов должен быть создан\"\n",
    "assert \"test_layer_bias\" in adam.v, \"Момент для bias должен быть создан\"\n",
    "\n",
    "# Проверим формы моментов\n",
    "assert adam.m[\"test_layer_weight\"].shape == layer.weight.shape, \"Неправильная форма момента весов\"\n",
    "assert adam.m[\"test_layer_bias\"].shape == layer.bias.shape, \"Неправильная форма момента bias\"\n",
    "\n",
    "# Тест zero_grad\n",
    "adam.zero_grad([layer])\n",
    "assert layer.grad_weight is None, \"Градиенты весов должны быть обнулены\"\n",
    "assert layer.grad_bias is None, \"Градиенты bias должны быть обнулены\"\n",
    "\n",
    "print(\"✅ Adam тест пройден успешно!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Реализуйте Adam класс выше, затем раскомментируйте этот код для тестирования\n",
      "Initial weight: \n",
      "[[-0.66143495  0.14313142]\n",
      " [-0.07901996  1.03411187]\n",
      " [ 0.23777134 -0.32971502]]\n",
      "Initial bias: [0. 0.]\n",
      "Weight gradient: \n",
      "[[0.1 0.2]\n",
      " [0.3 0.4]\n",
      " [0.5 0.6]]\n",
      "Bias gradient: [0.1 0.2]\n",
      "\n",
      "After 1 step:\n",
      "Updated weight: \n",
      "[[-0.67143495  0.13313142]\n",
      " [-0.08901996  1.02411186]\n",
      " [ 0.22777134 -0.33971502]]\n",
      "Updated bias: [-0.01 -0.01]\n",
      "Time step: 1\n",
      "✅ Adam тест пройден успешно!\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Функции потерь\n",
    "\n",
    "### Теория\n",
    "\n",
    "**Функции потерь** измеряют разность между предсказаниями модели и истинными значениями.\n",
    "\n",
    "**Cross-Entropy Loss:**\n",
    "- Используется для задач классификации\n",
    "- Формула: `-log(p_correct_class)`\n",
    "- Штрафует неверные предсказания экспоненциально\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "- Используется для задач регрессии\n",
    "- Формула: `(y_pred - y_true)²`\n",
    "- Чувствителен к выбросам\n",
    "\n",
    "### Реализация\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:37.498071Z",
     "start_time": "2025-08-12T18:07:37.492178Z"
    }
   },
   "source": [
    "class CrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        self.predictions = None\n",
    "        self.targets = None\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Вычисление Cross-Entropy Loss\n",
    "\n",
    "        Args:\n",
    "            predictions: предсказания модели (batch_size, num_classes)\n",
    "            targets: истинные метки класса (batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            значение функции потерь\n",
    "        \"\"\"\n",
    "        self.predictions = softmax(predictions)\n",
    "        self.targets = targets\n",
    "\n",
    "        batch_size = predictions.shape[0]\n",
    "        log_likelihood = -np.log(self.predictions[np.arange(batch_size), targets] + 1e-15)\n",
    "        loss = np.mean(log_likelihood)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Вычисление градиента Cross-Entropy Loss\n",
    "\n",
    "        Returns:\n",
    "            градиент по предсказаниям\n",
    "        \"\"\"\n",
    "        batch_size = self.predictions.shape[0]\n",
    "        grad = self.predictions.copy()\n",
    "        grad[np.arange(batch_size), self.targets] -= 1\n",
    "        grad /= batch_size\n",
    "        return grad\n",
    "\n",
    "\n",
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        self.predictions = None\n",
    "        self.targets = None\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        \"\"\"\n",
    "        Вычисление Mean Squared Error\n",
    "\n",
    "        Args:\n",
    "            predictions: предсказания модели\n",
    "            targets: истинные значения\n",
    "\n",
    "        Returns:\n",
    "            значение функции потерь\n",
    "        \"\"\"\n",
    "        self.predictions = predictions\n",
    "        self.targets = targets\n",
    "        loss = np.mean((predictions - targets) ** 2)\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Вычисление градиента MSE\n",
    "\n",
    "        Returns:\n",
    "            градиент по предсказаниям\n",
    "        \"\"\"\n",
    "        grad = 2 * (self.predictions - self.targets) / self.predictions.size\n",
    "        return grad\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Устойчивая реализация softmax\n",
    "    \"\"\"\n",
    "    shifted_x = x - np.max(x, axis=1, keepdims=True)\n",
    "    exp_x = np.exp(shifted_x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    \"\"\"\n",
    "    Преобразование меток в one-hot кодировку\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((labels.size, num_classes))\n",
    "    one_hot[np.arange(labels.size), labels] = 1\n",
    "    return one_hot\n"
   ],
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:37.658156Z",
     "start_time": "2025-08-12T18:07:37.647446Z"
    }
   },
   "source": [
    "# Тест функций потерь (запустите после реализации Loss функций)\n",
    "print(\"⚠️ Реализуйте функции потерь выше, затем раскомментируйте этот код для тестирования\")\n",
    "\n",
    "# Тест CrossEntropyLoss\n",
    "print(\"🔥 Тестирование CrossEntropyLoss...\")\n",
    "ce_loss = CrossEntropyLoss()\n",
    "\n",
    "# Тестовые данные\n",
    "predictions = np.array([[2.0, 1.0, 0.1], [1.0, 3.0, 0.2]], dtype=np.float32)\n",
    "targets = np.array([0, 1], dtype=np.int32)\n",
    "\n",
    "print(f\"Predictions: \\n{predictions}\")\n",
    "print(f\"Targets: {targets}\")\n",
    "\n",
    "# Forward pass\n",
    "loss_value = ce_loss.forward(predictions, targets)\n",
    "print(f\"CrossEntropy Loss: {loss_value:.4f}\")\n",
    "\n",
    "# Проверим, что loss положительный\n",
    "assert loss_value > 0, \"CrossEntropy loss должен быть положительным\"\n",
    "\n",
    "# Backward pass\n",
    "grad = ce_loss.backward()\n",
    "print(f\"Gradient shape: {grad.shape}\")\n",
    "print(f\"Gradient: \\n{grad}\")\n",
    "\n",
    "# Проверим форму градиента\n",
    "assert grad.shape == predictions.shape, \"Неправильная форма градиента CrossEntropy\"\n",
    "\n",
    "# Проверим, что сумма градиентов по классам равна 0 (свойство softmax)\n",
    "assert np.allclose(grad.sum(axis=1), 0, atol=1e-6), \"Сумма градиентов по классам должна быть 0\"\n",
    "\n",
    "print(\"✅ CrossEntropyLoss тест пройден!\")\n",
    "\n",
    "# Тест MSELoss\n",
    "print(\"\\n📊 Тестирование MSELoss...\")\n",
    "mse_loss = MSELoss()\n",
    "\n",
    "# Тестовые данные для регрессии\n",
    "predictions_reg = np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32)\n",
    "targets_reg = np.array([[1.5, 2.5], [2.5, 3.5]], dtype=np.float32)\n",
    "\n",
    "print(f\"Predictions: \\n{predictions_reg}\")\n",
    "print(f\"Targets: \\n{targets_reg}\")\n",
    "\n",
    "# Forward pass\n",
    "mse_value = mse_loss.forward(predictions_reg, targets_reg)\n",
    "print(f\"MSE Loss: {mse_value:.4f}\")\n",
    "\n",
    "# Проверим, что loss положительный\n",
    "assert mse_value >= 0, \"MSE loss должен быть неотрицательным\"\n",
    "\n",
    "# Backward pass\n",
    "grad_mse = mse_loss.backward()\n",
    "print(f\"MSE Gradient shape: {grad_mse.shape}\")\n",
    "print(f\"MSE Gradient: \\n{grad_mse}\")\n",
    "\n",
    "# Проверим форму градиента\n",
    "assert grad_mse.shape == predictions_reg.shape, \"Неправильная форма градиента MSE\"\n",
    "\n",
    "print(\"✅ MSELoss тест пройден!\")\n",
    "\n",
    "# Тест softmax функции\n",
    "print(\"\\n🎯 Тестирование Softmax...\")\n",
    "x_softmax = np.array([[1.0, 2.0, 3.0], [1.0, 1.0, 1.0]], dtype=np.float32)\n",
    "softmax_output = softmax(x_softmax)\n",
    "\n",
    "print(f\"Input: \\n{x_softmax}\")\n",
    "print(f\"Softmax output: \\n{softmax_output}\")\n",
    "\n",
    "# Проверим, что сумма вероятностей равна 1\n",
    "assert np.allclose(softmax_output.sum(axis=1), 1.0), \"Сумма softmax должна быть равна 1\"\n",
    "\n",
    "# Проверим, что все значения положительные\n",
    "assert np.all(softmax_output > 0), \"Все значения softmax должны быть положительными\"\n",
    "assert np.all(softmax_output < 1), \"Все значения softmax должны быть меньше 1\"\n",
    "\n",
    "print(\"✅ Softmax тест пройден!\")\n",
    "\n",
    "# Тест one-hot encoding\n",
    "print(\"\\n🏷️ Тестирование One-hot encoding...\")\n",
    "labels = np.array([0, 2, 1, 0])\n",
    "one_hot = one_hot_encode(labels, num_classes=3)\n",
    "\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"One-hot: \\n{one_hot}\")\n",
    "\n",
    "# Проверим форму\n",
    "assert one_hot.shape == (4, 3), \"Неправильная форма one-hot кодировки\"\n",
    "\n",
    "# Проверим, что каждая строка содержит ровно одну единицу\n",
    "assert np.all(one_hot.sum(axis=1) == 1), \"Каждая строка должна содержать ровно одну единицу\"\n",
    "\n",
    "print(\"✅ One-hot encoding тест пройден!\")\n",
    "\n",
    "print(\"\\n🎉 Все тесты функций потерь пройдены успешно!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Реализуйте функции потерь выше, затем раскомментируйте этот код для тестирования\n",
      "🔥 Тестирование CrossEntropyLoss...\n",
      "Predictions: \n",
      "[[2.  1.  0.1]\n",
      " [1.  3.  0.2]]\n",
      "Targets: [0 1]\n",
      "CrossEntropy Loss: 0.2981\n",
      "Gradient shape: (2, 3)\n",
      "Gradient: \n",
      "[[-0.17049944  0.12121648  0.04928295]\n",
      " [ 0.05657142 -0.08199063  0.02541918]]\n",
      "✅ CrossEntropyLoss тест пройден!\n",
      "\n",
      "📊 Тестирование MSELoss...\n",
      "Predictions: \n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "Targets: \n",
      "[[1.5 2.5]\n",
      " [2.5 3.5]]\n",
      "MSE Loss: 0.2500\n",
      "MSE Gradient shape: (2, 2)\n",
      "MSE Gradient: \n",
      "[[-0.25 -0.25]\n",
      " [ 0.25  0.25]]\n",
      "✅ MSELoss тест пройден!\n",
      "\n",
      "🎯 Тестирование Softmax...\n",
      "Input: \n",
      "[[1. 2. 3.]\n",
      " [1. 1. 1.]]\n",
      "Softmax output: \n",
      "[[0.09003057 0.24472846 0.66524094]\n",
      " [0.33333334 0.33333334 0.33333334]]\n",
      "✅ Softmax тест пройден!\n",
      "\n",
      "🏷️ Тестирование One-hot encoding...\n",
      "Labels: [0 2 1 0]\n",
      "One-hot: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "✅ One-hot encoding тест пройден!\n",
      "\n",
      "🎉 Все тесты функций потерь пройдены успешно!\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Обновленная архитектура нейронной сети\n",
    "\n",
    "Теперь создайте нейронную сеть с использованием всех реализованных компонентов."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:54.890256Z",
     "start_time": "2025-08-12T18:07:38.341006Z"
    }
   },
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.model = Sequential(\n",
    "            Linear(784, 256),\n",
    "            BatchNorm(256),\n",
    "            Dropout(0.3),\n",
    "            Linear(256, 128),\n",
    "            BatchNorm(128),\n",
    "            Dropout(0.3),\n",
    "            Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return self.model.backward(grad_output)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_trainable_layers(self):\n",
    "        trainable_layers = []\n",
    "        for layer in self.model.layers:\n",
    "            if hasattr(layer, 'update_weights'):\n",
    "                trainable_layers.append(layer)\n",
    "        return trainable_layers\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    data = pd.read_csv(\"data/train.csv\")\n",
    "    X = data.iloc[:, 1:].values / 255.0\n",
    "    y = data.iloc[:, 0].values\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = NeuralNetwork()\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    epochs = 10\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        outputs = model.forward(X_train)\n",
    "        loss = loss_fn.forward(outputs, y_train)\n",
    "\n",
    "        grad = loss_fn.backward()\n",
    "        model.backward(grad)\n",
    "\n",
    "        for i, layer in enumerate(model.get_trainable_layers()):\n",
    "            optimizer.update(layer, i)\n",
    "\n",
    "        optimizer.zero_grad(model.get_trainable_layers())\n",
    "\n",
    "        model.eval()\n",
    "        val_outputs = model.forward(X_val)\n",
    "        val_loss = loss_fn.forward(val_outputs, y_val)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 3.4655, Val Loss: 2.0605\n",
      "Epoch 2/10, Train Loss: 2.4369, Val Loss: 1.7687\n",
      "Epoch 3/10, Train Loss: 1.8702, Val Loss: 1.5162\n",
      "Epoch 4/10, Train Loss: 1.4629, Val Loss: 1.2991\n",
      "Epoch 5/10, Train Loss: 1.1869, Val Loss: 1.1183\n",
      "Epoch 6/10, Train Loss: 0.9893, Val Loss: 0.9710\n",
      "Epoch 7/10, Train Loss: 0.8602, Val Loss: 0.8532\n",
      "Epoch 8/10, Train Loss: 0.7596, Val Loss: 0.7587\n",
      "Epoch 9/10, Train Loss: 0.6896, Val Loss: 0.6827\n",
      "Epoch 10/10, Train Loss: 0.6288, Val Loss: 0.6217\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:07:54.900255Z",
     "start_time": "2025-08-12T18:07:54.897256Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Отдельное под Kaggle"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T18:10:35.898425Z",
     "start_time": "2025-08-12T18:09:06.224066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.model = Sequential(\n",
    "            Linear(784, 256),\n",
    "            BatchNorm(256),\n",
    "            ReLU(),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            Linear(256, 128),\n",
    "            BatchNorm(128),\n",
    "            ReLU(),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            Linear(128, 64),\n",
    "            BatchNorm(64),\n",
    "            ReLU(),\n",
    "            Dropout(0.2),\n",
    "\n",
    "            Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return self.model.backward(grad_output)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_trainable_layers(self):\n",
    "        return [layer for layer in self.model.layers if hasattr(layer, 'update_weights')]\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "X = train_df.iloc[:, 1:].values / 255.0\n",
    "y = train_df.iloc[:, 0].values\n",
    "X_test = test_df.values / 255.0\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = NeuralNetwork()\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    permutation = np.random.permutation(len(X_train))\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        idx = permutation[i:i+batch_size]\n",
    "        xb = X_train[idx]\n",
    "        yb = y_train[idx]\n",
    "\n",
    "        outputs = model.forward(xb)\n",
    "        loss = loss_fn.forward(outputs, yb)\n",
    "        grad = loss_fn.backward()\n",
    "        model.backward(grad)\n",
    "\n",
    "        for lid, layer in enumerate(model.get_trainable_layers()):\n",
    "            optimizer.update(layer, lid)\n",
    "        optimizer.zero_grad(model.get_trainable_layers())\n",
    "\n",
    "        epoch_loss += loss * len(xb)\n",
    "\n",
    "    model.eval()\n",
    "    val_outputs = model.forward(X_val)\n",
    "    val_pred = np.argmax(softmax(val_outputs), axis=1)\n",
    "    val_acc = np.mean(val_pred == y_val)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss/len(X_train):.4f} - Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "test_outputs = model.forward(X_test)\n",
    "test_pred = np.argmax(softmax(test_outputs), axis=1)\n",
    "\n",
    "submission = pd.DataFrame({\"ImageId\": np.arange(1, len(test_pred) + 1), \"Label\": test_pred})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 0.3769 - Val Acc: 0.9581\n",
      "Epoch 2/20 - Loss: 0.1801 - Val Acc: 0.9640\n",
      "Epoch 3/20 - Loss: 0.1429 - Val Acc: 0.9686\n",
      "Epoch 4/20 - Loss: 0.1201 - Val Acc: 0.9705\n",
      "Epoch 5/20 - Loss: 0.1078 - Val Acc: 0.9769\n",
      "Epoch 6/20 - Loss: 0.0965 - Val Acc: 0.9762\n",
      "Epoch 7/20 - Loss: 0.0864 - Val Acc: 0.9762\n",
      "Epoch 8/20 - Loss: 0.0845 - Val Acc: 0.9774\n",
      "Epoch 9/20 - Loss: 0.0753 - Val Acc: 0.9755\n",
      "Epoch 10/20 - Loss: 0.0704 - Val Acc: 0.9755\n",
      "Epoch 11/20 - Loss: 0.0696 - Val Acc: 0.9762\n",
      "Epoch 12/20 - Loss: 0.0639 - Val Acc: 0.9760\n",
      "Epoch 13/20 - Loss: 0.0627 - Val Acc: 0.9812\n",
      "Epoch 14/20 - Loss: 0.0585 - Val Acc: 0.9769\n",
      "Epoch 15/20 - Loss: 0.0533 - Val Acc: 0.9790\n",
      "Epoch 16/20 - Loss: 0.0521 - Val Acc: 0.9764\n",
      "Epoch 17/20 - Loss: 0.0532 - Val Acc: 0.9788\n",
      "Epoch 18/20 - Loss: 0.0460 - Val Acc: 0.9767\n",
      "Epoch 19/20 - Loss: 0.0510 - Val Acc: 0.9774\n",
      "Epoch 20/20 - Loss: 0.0499 - Val Acc: 0.9795\n",
      "Файл submission.csv сохранён\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
